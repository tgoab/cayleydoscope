{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "44799385",
      "metadata": {},
      "outputs": [],
      "source": [
        "import functools\n",
        "import os\n",
        "import shutil\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "import clrs\n",
        "import jax\n",
        "import numpy as np\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from jax import numpy as jnp\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "966a764e",
      "metadata": {},
      "outputs": [],
      "source": [
        "for name in list(flags.FLAGS):\n",
        "  delattr(flags.FLAGS, name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13aaf06e",
      "metadata": {},
      "outputs": [],
      "source": [
        "flags.DEFINE_list('algorithms', ['dfs'], 'Which algorithms to run.')\n",
        "flags.DEFINE_list('train_lengths', ['4', '7', '11', '13', '16'],\n",
        "                  'Which training sizes to use. A size of -1 means '\n",
        "                  'use the benchmark dataset.')\n",
        "flags.DEFINE_integer('length_needle', -8,\n",
        "                     'Length of needle for training and validation '\n",
        "                     '(not testing) in string matching algorithms. '\n",
        "                     'A negative value randomizes the length for each sample '\n",
        "                     'between 1 and the opposite of the value. '\n",
        "                     'A value of 0 means use always 1/4 of the length of '\n",
        "                     'the haystack (the default sampler behavior).')\n",
        "flags.DEFINE_integer('seed', 42, 'Random seed to set')\n",
        "\n",
        "flags.DEFINE_boolean('random_pos', True,\n",
        "                     'Randomize the pos input common to all algos.')\n",
        "flags.DEFINE_boolean('enforce_permutations', True,\n",
        "                     'Whether to enforce permutation-type node pointers.')\n",
        "flags.DEFINE_boolean('enforce_pred_as_input', True,\n",
        "                     'Whether to change pred_h hints into pred inputs.')\n",
        "flags.DEFINE_integer('batch_size', 32, 'Batch size used for training.')\n",
        "flags.DEFINE_boolean('chunked_training', False,\n",
        "                     'Whether to use chunking for training.')\n",
        "flags.DEFINE_integer('chunk_length', 16,\n",
        "                     'Time chunk length used for training (if '\n",
        "                     '`chunked_training` is True.')\n",
        "flags.DEFINE_integer('train_steps', 200, 'Number of training iterations.')\n",
        "flags.DEFINE_integer('eval_every', 10, 'Evaluation frequency (in steps).')\n",
        "flags.DEFINE_integer('test_every', 500, 'Evaluation frequency (in steps).')\n",
        "\n",
        "flags.DEFINE_integer('hidden_size', 128,\n",
        "                     'Number of hidden units of the model.')\n",
        "flags.DEFINE_integer('nb_heads', 1, 'Number of heads for GAT processors')\n",
        "flags.DEFINE_integer('nb_msg_passing_steps', 1,\n",
        "                     'Number of message passing steps to run per hint.')\n",
        "flags.DEFINE_float('learning_rate', 0.001, 'Learning rate to use.')\n",
        "flags.DEFINE_float('grad_clip_max_norm', 1.0,\n",
        "                   'Gradient clipping by norm. 0.0 disables grad clipping')\n",
        "flags.DEFINE_float('dropout_prob', 0.0, 'Dropout rate to use.')\n",
        "flags.DEFINE_float('hint_teacher_forcing', 0.0,\n",
        "                   'Probability that ground-truth teacher hints are encoded '\n",
        "                   'during training instead of predicted hints. Only '\n",
        "                   'pertinent in encoded_decoded modes.')\n",
        "flags.DEFINE_enum('hint_mode', 'encoded_decoded',\n",
        "                  ['encoded_decoded', 'decoded_only', 'none'],\n",
        "                  'How should hints be used? Note, each mode defines a '\n",
        "                  'separate task, with various difficulties. `encoded_decoded` '\n",
        "                  'requires the model to explicitly materialise hint sequences '\n",
        "                  'and therefore is hardest, but also most aligned to the '\n",
        "                  'underlying algorithmic rule. Hence, `encoded_decoded` '\n",
        "                  'should be treated as the default mode for our benchmark. '\n",
        "                  'In `decoded_only`, hints are only used for defining '\n",
        "                  'reconstruction losses. Often, this will perform well, but '\n",
        "                  'note that we currently do not make any efforts to '\n",
        "                  'counterbalance the various hint losses. Hence, for certain '\n",
        "                  'tasks, the best performance will now be achievable with no '\n",
        "                  'hint usage at all (`none`).')\n",
        "flags.DEFINE_enum('hint_repred_mode', 'soft', ['soft', 'hard', 'hard_on_eval'],\n",
        "                  'How to process predicted hints when fed back as inputs.'\n",
        "                  'In soft mode, we use softmaxes for categoricals, pointers '\n",
        "                  'and mask_one, and sigmoids for masks. '\n",
        "                  'In hard mode, we use argmax instead of softmax, and hard '\n",
        "                  'thresholding of masks. '\n",
        "                  'In hard_on_eval mode, soft mode is '\n",
        "                  'used for training and hard mode is used for evaluation.')\n",
        "flags.DEFINE_boolean('use_ln', True,\n",
        "                     'Whether to use layer normalisation in the processor.')\n",
        "flags.DEFINE_boolean('use_lstm', False,\n",
        "                     'Whether to insert an LSTM after message passing.')\n",
        "flags.DEFINE_integer('nb_triplet_fts', 8,\n",
        "                     'How many triplet features to compute?')\n",
        "\n",
        "flags.DEFINE_enum('encoder_init', 'xavier_on_scalars',\n",
        "                  ['default', 'xavier_on_scalars'],\n",
        "                  'Initialiser to use for the encoders.')\n",
        "flags.DEFINE_enum('processor_type', 'mpnn',\n",
        "                  ['deepsets', 'mpnn', 'pgn', 'pgn_mask',\n",
        "                   'triplet_mpnn', 'triplet_pgn', 'triplet_pgn_mask',\n",
        "                   'gat', 'gatv2', 'gat_full', 'gatv2_full',\n",
        "                   'gpgn', 'gpgn_mask', 'gmpnn',\n",
        "                   'triplet_gpgn', 'triplet_gpgn_mask', 'triplet_gmpnn'],\n",
        "                  'Processor type to use as the network P.')\n",
        "\n",
        "flags.DEFINE_string('checkpoint_path', '/tmp/CLRS30',\n",
        "                    'Path in which checkpoints are saved.')\n",
        "flags.DEFINE_string('dataset_path', '/tmp/CLRS30',\n",
        "                    'Path in which dataset is stored.')\n",
        "flags.DEFINE_boolean('freeze_processor', False,\n",
        "                     'Whether to freeze the processor of the model.')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "PRED_AS_INPUT_ALGOS = [\n",
        "    'binary_search',\n",
        "    'minimum',\n",
        "    'find_maximum_subarray',\n",
        "    'find_maximum_subarray_kadane',\n",
        "    'matrix_chain_order',\n",
        "    'lcs_length',\n",
        "    'optimal_bst',\n",
        "    'activity_selector',\n",
        "    'task_scheduling',\n",
        "    'naive_string_matcher',\n",
        "    'kmp_matcher',\n",
        "    'jarvis_march']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "60e24a58",
      "metadata": {},
      "outputs": [],
      "source": [
        "flags.FLAGS.mark_as_parsed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d5b17067",
      "metadata": {},
      "outputs": [],
      "source": [
        "# helpers\n",
        "def unpack(v):\n",
        "  try:\n",
        "    return v.item()  # DeviceArray  # pytype: disable=attribute-error\n",
        "  except (AttributeError, ValueError):\n",
        "    return v\n",
        "\n",
        "\n",
        "def _iterate_sampler(sampler, batch_size):\n",
        "  while True:\n",
        "    yield sampler.next(batch_size)\n",
        "\n",
        "\n",
        "def _maybe_download_dataset(dataset_path):\n",
        "  \"\"\"Download CLRS30 dataset if needed.\"\"\"\n",
        "  dataset_folder = os.path.join(dataset_path, clrs.get_clrs_folder())\n",
        "  if os.path.isdir(dataset_folder):\n",
        "    logging.info('Dataset found at %s. Skipping download.', dataset_folder)\n",
        "    return dataset_folder\n",
        "  logging.info('Dataset not found in %s. Downloading...', dataset_folder)\n",
        "\n",
        "  clrs_url = clrs.get_dataset_gcp_url()\n",
        "  request = requests.get(clrs_url, allow_redirects=True)\n",
        "  clrs_file = os.path.join(dataset_path, os.path.basename(clrs_url))\n",
        "  os.makedirs(dataset_folder)\n",
        "  open(clrs_file, 'wb').write(request.content)\n",
        "  shutil.unpack_archive(clrs_file, extract_dir=dataset_folder)\n",
        "  os.remove(clrs_file)\n",
        "  return dataset_folder\n",
        "\n",
        "\n",
        "def _concat(dps, axis):\n",
        "  return jax.tree_util.tree_map(lambda *x: np.concatenate(x, axis), *dps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "718b1080",
      "metadata": {},
      "outputs": [],
      "source": [
        "# samplers and eval stuff\n",
        "def make_sampler(length: int,\n",
        "                 rng: Any,\n",
        "                 algorithm: str,\n",
        "                 split: str,\n",
        "                 batch_size: int,\n",
        "                 multiplier: int,\n",
        "                 randomize_pos: bool,\n",
        "                 enforce_pred_as_input: bool,\n",
        "                 enforce_permutations: bool,\n",
        "                 chunked: bool,\n",
        "                 chunk_length: int,\n",
        "                 sampler_kwargs: Dict[str, Any]):\n",
        "  \"\"\"Create a sampler with given options.\n",
        "\n",
        "  Args:\n",
        "    length: Size of samples (i.e., number of nodes in the graph).\n",
        "      A length of -1 will mean that the benchmark\n",
        "      dataset (for the given split) is used. Positive sizes will instantiate\n",
        "      samplers of the corresponding size.\n",
        "    rng: Numpy random state.\n",
        "    algorithm: The name of the algorithm to sample from.\n",
        "    split: 'train', 'val' or 'test'.\n",
        "    batch_size: Samples per batch.\n",
        "    multiplier: Integer multiplier for the number of samples in the dataset,\n",
        "      only used for positive sizes. Negative multiplier means infinite samples.\n",
        "    randomize_pos: Whether to randomize the `pos` input.\n",
        "    enforce_pred_as_input: Whether to convert fixed pred_h hints to inputs.\n",
        "    enforce_permutations: Whether to enforce permutation pointers.\n",
        "    chunked: Whether to chunk the dataset.\n",
        "    chunk_length: Unroll length of chunks, if `chunked` is True.\n",
        "    sampler_kwargs: Extra args passed to the sampler.\n",
        "  Returns:\n",
        "    A sampler (iterator), the number of samples in the iterator (negative\n",
        "    if infinite samples), and the spec.\n",
        "  \"\"\"\n",
        "  if length < 0:  # load from file\n",
        "    dataset_folder = _maybe_download_dataset(FLAGS.dataset_path)\n",
        "    sampler, num_samples, spec = clrs.create_dataset(folder=dataset_folder,\n",
        "                                                     algorithm=algorithm,\n",
        "                                                     batch_size=batch_size,\n",
        "                                                     split=split)\n",
        "    sampler = sampler.as_numpy_iterator()\n",
        "  else:\n",
        "    num_samples = clrs.CLRS30[split]['num_samples'] * multiplier\n",
        "    sampler, spec = clrs.build_sampler(\n",
        "        algorithm,\n",
        "        seed=rng.randint(2**32),\n",
        "        num_samples=num_samples,\n",
        "        length=length,\n",
        "        **sampler_kwargs,\n",
        "        )\n",
        "    sampler = _iterate_sampler(sampler, batch_size)\n",
        "\n",
        "  if randomize_pos:\n",
        "    sampler = clrs.process_random_pos(sampler, rng)\n",
        "  if enforce_pred_as_input and algorithm in PRED_AS_INPUT_ALGOS:\n",
        "    spec, sampler = clrs.process_pred_as_input(spec, sampler)\n",
        "  spec, sampler = clrs.process_permutations(spec, sampler, enforce_permutations)\n",
        "  if chunked:\n",
        "    sampler = clrs.chunkify(sampler, chunk_length)\n",
        "  return sampler, num_samples, spec\n",
        "\n",
        "\n",
        "def make_multi_sampler(sizes, rng, **kwargs):\n",
        "  \"\"\"Create a sampler with cycling sample sizes.\"\"\"\n",
        "  ss = []\n",
        "  tot_samples = 0\n",
        "  for length in sizes:\n",
        "    sampler, num_samples, spec = make_sampler(length, rng, **kwargs)\n",
        "    ss.append(sampler)\n",
        "    tot_samples += num_samples\n",
        "\n",
        "  def cycle_samplers():\n",
        "    while True:\n",
        "      for s in ss:\n",
        "        yield next(s)\n",
        "  return cycle_samplers(), tot_samples, spec\n",
        "\n",
        "\n",
        "def _concat(dps, axis):\n",
        "  return jax.tree_util.tree_map(lambda *x: np.concatenate(x, axis), *dps)\n",
        "\n",
        "\n",
        "def collect_and_eval(sampler, predict_fn, sample_count, rng_key, extras):\n",
        "  \"\"\"Collect batches of output and hint preds and evaluate them.\"\"\"\n",
        "  processed_samples = 0\n",
        "  preds = []\n",
        "  outputs = []\n",
        "  while processed_samples < sample_count:\n",
        "    feedback = next(sampler)\n",
        "    batch_size = feedback.outputs[0].data.shape[0]\n",
        "    outputs.append(feedback.outputs)\n",
        "    new_rng_key, rng_key = jax.random.split(rng_key)\n",
        "    cur_preds, _ = predict_fn(new_rng_key, feedback.features)\n",
        "    preds.append(cur_preds)\n",
        "    processed_samples += batch_size\n",
        "  outputs = _concat(outputs, axis=0)\n",
        "  preds = _concat(preds, axis=0)\n",
        "  out = clrs.evaluate(outputs, preds)\n",
        "  if extras:\n",
        "    out.update(extras)\n",
        "  return {k: unpack(v) for k, v in out.items()}\n",
        "\n",
        "\n",
        "def create_samplers(\n",
        "    rng,\n",
        "    train_lengths: List[int],\n",
        "    *,\n",
        "    algorithms: Optional[List[str]] = None,\n",
        "    val_lengths: Optional[List[int]] = None,\n",
        "    test_lengths: Optional[List[int]] = None,\n",
        "    train_batch_size: int = 32,\n",
        "    val_batch_size: int = 32,\n",
        "    test_batch_size: int = 32,\n",
        "):\n",
        "  \"\"\"Create samplers for training, validation and testing.\n",
        "\n",
        "  Args:\n",
        "    rng: Numpy random state.\n",
        "    train_lengths: list of training lengths to use for each algorithm.\n",
        "    algorithms: list of algorithms to generate samplers for. Set to\n",
        "        FLAGS.algorithms if not provided.\n",
        "    val_lengths: list of lengths for validation samplers for each algorithm. Set\n",
        "        to maxumim training length if not provided.\n",
        "    test_lengths: list of lengths for test samplers for each algorithm. Set to\n",
        "        [-1] to use the benchmark dataset if not provided.\n",
        "    train_batch_size: batch size for training samplers.\n",
        "    val_batch_size: batch size for validation samplers.\n",
        "    test_batch_size: batch size for test samplers.\n",
        "\n",
        "  Returns:\n",
        "    Tuple of:\n",
        "      train_samplers: list of samplers for training.\n",
        "      val_samplers: list of samplers for validation.\n",
        "      val_sample_counts: list of sample counts for validation.\n",
        "      test_samplers: list of samplers for testing.\n",
        "      test_sample_counts: list of sample counts for testing.\n",
        "      spec_list: list of specs for each algorithm.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  train_samplers = []\n",
        "  val_samplers = []\n",
        "  val_sample_counts = []\n",
        "  test_samplers = []\n",
        "  test_sample_counts = []\n",
        "  spec_list = []\n",
        "\n",
        "  algorithms = algorithms or FLAGS.algorithms\n",
        "  for algo_idx, algorithm in enumerate(algorithms):\n",
        "    # Set the training lengths for the current algorithm.\n",
        "    current_algo_train_lengths = train_lengths\n",
        "\n",
        "     # Make full dataset pipeline run on CPU (including prefetching).\n",
        "    with tf.device('/cpu:0'):\n",
        "      if algorithm in ['naive_string_matcher', 'kmp_matcher']:\n",
        "        # Fixed haystack + needle; variability will be in needle\n",
        "        # Still, for chunked training, we maintain as many samplers\n",
        "        # as train lengths, since, for each length there is a separate state,\n",
        "        # and we must keep the 1:1 relationship between states and samplers.\n",
        "        max_length = max(current_algo_train_lengths)\n",
        "        if max_length > 0:  # if < 0, we are using the benchmark data\n",
        "          max_length = (max_length * 5) // 4\n",
        "        current_algo_train_lengths = [max_length]\n",
        "        if FLAGS.chunked_training:\n",
        "          current_algo_train_lengths = current_algo_train_lengths * len(\n",
        "              current_algo_train_lengths\n",
        "          )\n",
        "\n",
        "      logging.info('Creating samplers for algo %s', algorithm)\n",
        "\n",
        "      p = tuple([0.1 + 0.1 * i for i in range(9)])\n",
        "      if p and algorithm in ['articulation_points', 'bridges',\n",
        "                             'mst_kruskal', 'bipartite_matching']:\n",
        "        # Choose a lower connection probability for the above algorithms,\n",
        "        # otherwise trajectories are very long\n",
        "        p = tuple(np.array(p) / 2)\n",
        "      length_needle = FLAGS.length_needle\n",
        "      sampler_kwargs = dict(p=p, length_needle=length_needle)\n",
        "      if length_needle == 0:\n",
        "        sampler_kwargs.pop('length_needle')\n",
        "\n",
        "      common_sampler_args = dict(\n",
        "          algorithm=algorithms[algo_idx],\n",
        "          rng=rng,\n",
        "          enforce_pred_as_input=FLAGS.enforce_pred_as_input,\n",
        "          enforce_permutations=FLAGS.enforce_permutations,\n",
        "          chunk_length=FLAGS.chunk_length,\n",
        "          )\n",
        "\n",
        "      train_args = dict(\n",
        "          sizes=current_algo_train_lengths,\n",
        "          split='train',\n",
        "          batch_size=train_batch_size,\n",
        "          multiplier=-1,\n",
        "          randomize_pos=FLAGS.random_pos,\n",
        "          chunked=FLAGS.chunked_training,\n",
        "          sampler_kwargs=sampler_kwargs,\n",
        "          **common_sampler_args,\n",
        "      )\n",
        "      train_sampler, _, _ = make_multi_sampler(**train_args)\n",
        "\n",
        "      mult = clrs.CLRS_30_ALGS_SETTINGS[algorithm]['num_samples_multiplier']\n",
        "      val_args = dict(\n",
        "          sizes=val_lengths or [np.amax(current_algo_train_lengths)],\n",
        "          split='val',\n",
        "          batch_size=val_batch_size,\n",
        "          multiplier=2 * mult,\n",
        "          randomize_pos=FLAGS.random_pos,\n",
        "          chunked=False,\n",
        "          sampler_kwargs=sampler_kwargs,\n",
        "          **common_sampler_args,\n",
        "      )\n",
        "      val_sampler, val_samples, _ = make_multi_sampler(**val_args)\n",
        "\n",
        "      test_args = dict(sizes=test_lengths or [-1],\n",
        "                       split='test',\n",
        "                       batch_size=test_batch_size,\n",
        "                       multiplier=2 * mult,\n",
        "                       randomize_pos=False,\n",
        "                       chunked=False,\n",
        "                       sampler_kwargs={},\n",
        "                       **common_sampler_args)\n",
        "      test_sampler, test_samples, spec = make_multi_sampler(**test_args)\n",
        "\n",
        "    spec_list.append(spec)\n",
        "    train_samplers.append(train_sampler)\n",
        "    val_samplers.append(val_sampler)\n",
        "    val_sample_counts.append(val_samples)\n",
        "    test_samplers.append(test_sampler)\n",
        "    test_sample_counts.append(test_samples)\n",
        "\n",
        "  return (train_samplers,\n",
        "          val_samplers, val_sample_counts,\n",
        "          test_samplers, test_sample_counts,\n",
        "          spec_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "883b1f93",
      "metadata": {},
      "outputs": [],
      "source": [
        "def vanilla_clrs(seed, unused_argv):\n",
        "  wandb.init(project=\"clrs-project\", name=f\"baseline-training-{FLAGS.algorithms}-mpnn\")\n",
        "  if FLAGS.hint_mode == 'encoded_decoded':\n",
        "    encode_hints = True\n",
        "    decode_hints = True\n",
        "  elif FLAGS.hint_mode == 'decoded_only':\n",
        "    encode_hints = False\n",
        "    decode_hints = True\n",
        "  elif FLAGS.hint_mode == 'none':\n",
        "    encode_hints = False\n",
        "    decode_hints = False\n",
        "  else:\n",
        "    raise ValueError('Hint mode not in {encoded_decoded, decoded_only, none}.')\n",
        "\n",
        "  train_lengths = [int(x) for x in FLAGS.train_lengths]\n",
        "\n",
        "  rng = np.random.RandomState(seed)\n",
        "  rng_key = jax.random.PRNGKey(rng.randint(2**32))\n",
        "\n",
        "  # Create samplers\n",
        "  (\n",
        "      train_samplers,\n",
        "      val_samplers,\n",
        "      val_sample_counts,\n",
        "      test_samplers,\n",
        "      test_sample_counts,\n",
        "      spec_list,\n",
        "  ) = create_samplers(\n",
        "      rng=rng,\n",
        "      train_lengths=train_lengths,\n",
        "      algorithms=FLAGS.algorithms,\n",
        "      val_lengths=[np.amax(train_lengths)],\n",
        "      test_lengths=[-1],\n",
        "      train_batch_size=FLAGS.batch_size,\n",
        "  )\n",
        "\n",
        "  processor_factory = clrs.get_processor_factory(\n",
        "      FLAGS.processor_type,\n",
        "      use_ln=FLAGS.use_ln,\n",
        "      nb_triplet_fts=FLAGS.nb_triplet_fts,\n",
        "      nb_heads=FLAGS.nb_heads,\n",
        "  )\n",
        "  model_params = dict(\n",
        "      processor_factory=processor_factory,\n",
        "      hidden_dim=FLAGS.hidden_size,\n",
        "      encode_hints=encode_hints,\n",
        "      decode_hints=decode_hints,\n",
        "      encoder_init=FLAGS.encoder_init,\n",
        "      use_lstm=FLAGS.use_lstm,\n",
        "      learning_rate=FLAGS.learning_rate,\n",
        "      grad_clip_max_norm=FLAGS.grad_clip_max_norm,\n",
        "      checkpoint_path=FLAGS.checkpoint_path,\n",
        "      freeze_processor=FLAGS.freeze_processor,\n",
        "      dropout_prob=FLAGS.dropout_prob,\n",
        "      hint_teacher_forcing=FLAGS.hint_teacher_forcing,\n",
        "      hint_repred_mode=FLAGS.hint_repred_mode,\n",
        "      nb_msg_passing_steps=FLAGS.nb_msg_passing_steps,\n",
        "      )\n",
        "\n",
        "  eval_model = clrs.models.BaselineModel(\n",
        "      spec=spec_list,\n",
        "      dummy_trajectory=[next(t) for t in val_samplers],\n",
        "      **model_params\n",
        "  )\n",
        "  if FLAGS.chunked_training:\n",
        "    train_model = clrs.models.BaselineModelChunked(\n",
        "        spec=spec_list,\n",
        "        dummy_trajectory=[next(t) for t in train_samplers],\n",
        "        **model_params\n",
        "        )\n",
        "  else:\n",
        "    train_model = eval_model\n",
        "\n",
        "  # Training loop.\n",
        "  best_score = -1.0\n",
        "  current_train_items = [0] * len(FLAGS.algorithms)\n",
        "  step = 0\n",
        "  next_eval = 0\n",
        "  # Make sure scores improve on first step, but not overcome best score\n",
        "  # until all algos have had at least one evaluation.\n",
        "  val_scores = [-99999.9] * len(FLAGS.algorithms)\n",
        "  length_idx = 0\n",
        "\n",
        "  while step < FLAGS.train_steps:\n",
        "    feedback_list = [next(t) for t in train_samplers]\n",
        "\n",
        "    # Initialize model.\n",
        "    if step == 0:\n",
        "      all_features = [f.features for f in feedback_list]\n",
        "      if FLAGS.chunked_training:\n",
        "        # We need to initialize the model with samples of all lengths for\n",
        "        # all algorithms. Also, we need to make sure that the order of these\n",
        "        # sample sizes is the same as the order of the actual training sizes.\n",
        "        all_length_features = [all_features] + [\n",
        "            [next(t).features for t in train_samplers]\n",
        "            for _ in range(len(train_lengths))]\n",
        "        train_model.init(all_length_features[:-1], FLAGS.seed + 1)\n",
        "      else:\n",
        "        train_model.init(all_features, FLAGS.seed + 1)\n",
        "\n",
        "    # Training step.\n",
        "    print(f\"{train_samplers=}\")\n",
        "    for algo_idx in range(len(train_samplers)):\n",
        "      feedback = feedback_list[algo_idx]\n",
        "      rng_key, new_rng_key = jax.random.split(rng_key)\n",
        "      if FLAGS.chunked_training:\n",
        "        # In chunked training, we must indicate which training length we are\n",
        "        # using, so the model uses the correct state.\n",
        "        length_and_algo_idx = (length_idx, algo_idx)\n",
        "      else:\n",
        "        # In non-chunked training, all training lengths can be treated equally,\n",
        "        # since there is no state to maintain between batches.\n",
        "        length_and_algo_idx = algo_idx\n",
        "      cur_loss = train_model.feedback(rng_key, feedback, length_and_algo_idx)\n",
        "      rng_key = new_rng_key\n",
        "\n",
        "      if FLAGS.chunked_training:\n",
        "        examples_in_chunk = np.sum(feedback.features.is_last).item()\n",
        "      else:\n",
        "        examples_in_chunk = len(feedback.features.lengths)\n",
        "      current_train_items[algo_idx] += examples_in_chunk\n",
        "      logging.info('Algo %s step %i current loss %f, current_train_items %i.',\n",
        "                   FLAGS.algorithms[algo_idx], step,\n",
        "                   cur_loss, current_train_items[algo_idx])\n",
        "      wandb.log({\n",
        "          \"loss\": float(cur_loss),\n",
        "          \"step\": step\n",
        "      })\n",
        "\n",
        "    # Periodically evaluate model\n",
        "    if step >= next_eval:\n",
        "      eval_model.params = train_model.params\n",
        "      for algo_idx in range(len(train_samplers)):\n",
        "        common_extras = {'examples_seen': current_train_items[algo_idx],\n",
        "                         'step': step,\n",
        "                         'algorithm': FLAGS.algorithms[algo_idx]}\n",
        "\n",
        "        # Validation info.\n",
        "        new_rng_key, rng_key = jax.random.split(rng_key)\n",
        "        val_stats = collect_and_eval(\n",
        "            val_samplers[algo_idx],\n",
        "            functools.partial(eval_model.predict, algorithm_index=algo_idx),\n",
        "            val_sample_counts[algo_idx],\n",
        "            new_rng_key,\n",
        "            extras=common_extras)\n",
        "        logging.info('(val) algo %s step %d: %s',\n",
        "                     FLAGS.algorithms[algo_idx], step, val_stats)\n",
        "        val_scores[algo_idx] = val_stats['score']\n",
        "\n",
        "      next_eval += FLAGS.eval_every\n",
        "\n",
        "      # If best total score, update best checkpoint.\n",
        "      # Also save a best checkpoint on the first step.\n",
        "      msg = (f'best avg val score was '\n",
        "             f'{best_score/len(FLAGS.algorithms):.3f}, '\n",
        "             f'current avg val score is {np.mean(val_scores):.3f}, '\n",
        "             f'val scores are: ')\n",
        "      msg += ', '.join(\n",
        "          ['%s: %.3f' % (x, y) for (x, y) in zip(FLAGS.algorithms, val_scores)])\n",
        "      if (sum(val_scores) > best_score) or step == 0:\n",
        "        best_score = sum(val_scores)\n",
        "        logging.info('Checkpointing best model, %s', msg)\n",
        "        train_model.save_model('best.pkl')\n",
        "      else:\n",
        "        logging.info('Not saving new best model, %s', msg)\n",
        "    \n",
        "      wandb.log({\n",
        "          \"score\": float(np.mean(val_scores)),\n",
        "          \"step\": step\n",
        "      })\n",
        "\n",
        "    step += 1\n",
        "    length_idx = (length_idx + 1) % len(train_lengths)\n",
        "\n",
        "  logging.info('Restoring best model from checkpoint...')\n",
        "  eval_model.restore_model('best.pkl', only_load_processor=False)\n",
        "\n",
        "  for algo_idx in range(len(train_samplers)):\n",
        "    common_extras = {'examples_seen': current_train_items[algo_idx],\n",
        "                     'step': step,\n",
        "                     'algorithm': FLAGS.algorithms[algo_idx]}\n",
        "\n",
        "    new_rng_key, rng_key = jax.random.split(rng_key)\n",
        "    test_stats = collect_and_eval(\n",
        "        test_samplers[algo_idx],\n",
        "        functools.partial(eval_model.predict, algorithm_index=algo_idx),\n",
        "        test_sample_counts[algo_idx],\n",
        "        new_rng_key,\n",
        "        extras=common_extras)\n",
        "    logging.info('(test) algo %s : %s', FLAGS.algorithms[algo_idx], test_stats)\n",
        "\n",
        "  logging.info('Done!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6f172d27",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from /Users/juli/.netrc.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtgoab\u001b[0m (\u001b[33mtgoabike\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.24.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/juli/Documents/cs/machine learning/GNNs/cayleydoscope/cayleydoscope/clrs/examples/wandb/run-20260211_124339-v0ciznfi</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tgoabike/clrs-project/runs/v0ciznfi' target=\"_blank\">baseline-training-['kmp_matcher']-mpnn</a></strong> to <a href='https://wandb.ai/tgoabike/clrs-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/tgoabike/clrs-project' target=\"_blank\">https://wandb.ai/tgoabike/clrs-project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/tgoabike/clrs-project/runs/v0ciznfi' target=\"_blank\">https://wandb.ai/tgoabike/clrs-project/runs/v0ciznfi</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Ignoring kwargs {'p'} when building sampler class <class 'clrs._src.samplers.MatcherSampler'>\n",
            "WARNING:absl:Sampling dataset on-the-fly, unlimited samples.\n",
            "WARNING:absl:Ignoring kwargs {'p'} when building sampler class <class 'clrs._src.samplers.MatcherSampler'>\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n",
            "train_samplers=[<generator object make_multi_sampler.<locals>.cycle_samplers at 0x12bf11560>]\n"
          ]
        }
      ],
      "source": [
        "vanilla_clrs(42,'')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8543576a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "from typing import Dict\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "from clrs._src import processors\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Cayley Graph Utilities\n",
        "# ============================================================================\n",
        "\n",
        "def get_cayley_graph(n: int) -> np.ndarray:\n",
        "    \"\"\"Generate edge_index (2, E) of the Cayley graph Cay(SL(2, Z_n); S_n).\n",
        "\n",
        "    Uses 4 generators of SL(2, Z_n) and BFS from the identity matrix.\n",
        "    The generator set is closed under inversion, so the resulting graph\n",
        "    is effectively undirected.\n",
        "    \"\"\"\n",
        "    generators = np.array([\n",
        "        [[1, 1], [0, 1]],\n",
        "        [[1, n - 1], [0, 1]],\n",
        "        [[1, 0], [1, 1]],\n",
        "        [[1, 0], [n - 1, 1]],\n",
        "    ])\n",
        "    ind = 1\n",
        "    queue = deque([np.array([[1, 0], [0, 1]])])\n",
        "    nodes = {(1, 0, 0, 1): 0}\n",
        "    senders, receivers = [], []\n",
        "\n",
        "    while queue:\n",
        "        x = queue.pop()\n",
        "        x_flat = (x[0][0], x[0][1], x[1][0], x[1][1])\n",
        "        ind_x = nodes[x_flat]\n",
        "        for i in range(4):\n",
        "            tx = np.mod(np.matmul(x, generators[i]), n)\n",
        "            tx_flat = (tx[0][0], tx[0][1], tx[1][0], tx[1][1])\n",
        "            if tx_flat not in nodes:\n",
        "                nodes[tx_flat] = ind\n",
        "                ind += 1\n",
        "                queue.append(tx)\n",
        "            senders.append(ind_x)\n",
        "            receivers.append(nodes[tx_flat])\n",
        "\n",
        "    return np.asarray([senders, receivers], dtype=np.int64)\n",
        "\n",
        "\n",
        "def cayley_graph_size(n: int) -> int:\n",
        "    \"\"\"|SL(2, Z_n)| = n^3 * prod(1 - 1/p^2) for each distinct prime factor p.\"\"\"\n",
        "    from primefac import primefac\n",
        "    n = int(n)\n",
        "    if n <= 1:\n",
        "        return 1\n",
        "    primes = list(set(primefac(n)))\n",
        "    return round(n ** 3 * np.prod([1.0 - 1.0 / (p * p) for p in primes]))\n",
        "\n",
        "\n",
        "def get_cayley_n(num_nodes: int) -> int:\n",
        "    \"\"\"Smallest n such that |SL(2, Z_n)| >= num_nodes.\"\"\"\n",
        "    n = 1\n",
        "    while cayley_graph_size(n) < num_nodes:\n",
        "        n += 1\n",
        "    return n\n",
        "\n",
        "\n",
        "def build_truncated_cayley_adj(num_nodes: int) -> np.ndarray:\n",
        "    \"\"\"Build the EGP-style truncated Cayley expander adjacency matrix.\n",
        "\n",
        "    Steps:\n",
        "      1. Find smallest Cayley graph with >= num_nodes nodes\n",
        "      2. Generate full Cayley edge_index via BFS\n",
        "      3. Truncate: keep only edges where both endpoints < num_nodes\n",
        "      4. Convert to (num_nodes, num_nodes) adjacency matrix with self-loops\n",
        "\n",
        "    Returns:\n",
        "      np.ndarray of shape (num_nodes, num_nodes), dtype float32.\n",
        "    \"\"\"\n",
        "    n = get_cayley_n(num_nodes)\n",
        "    edge_index = get_cayley_graph(n)  # (2, E)\n",
        "\n",
        "    # Truncate to the input graph's number of nodes\n",
        "    mask = (edge_index[0] < num_nodes) & (edge_index[1] < num_nodes)\n",
        "    truncated = edge_index[:, mask]\n",
        "\n",
        "    # Build adjacency matrix\n",
        "    adj = np.zeros((num_nodes, num_nodes), dtype=np.float32)\n",
        "    adj[truncated[0], truncated[1]] = 1.0\n",
        "\n",
        "    # Add self-loops (consistent with PGN's initial adj_mat = eye(N))\n",
        "    np.fill_diagonal(adj, 1.0)\n",
        "\n",
        "    return adj\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EGP Processor\n",
        "# ============================================================================\n",
        "\n",
        "class EGP_Processor(processors.PGN):\n",
        "    \"\"\"Expander Graph Propagation processor using Cayley graphs.\n",
        "\n",
        "    Interweaves message passing over two graph topologies:\n",
        "      - Even time steps  original adj_mat  (the algorithm's actual graph)\n",
        "      - Odd  time steps  truncated Cayley expander adjacency\n",
        "\n",
        "    The Cayley adjacency is precomputed once per num_nodes and cached.\n",
        "    It is materialized as a JAX constant during tracing, so there is no\n",
        "    runtime overhead beyond the jnp.where selection.\n",
        "\n",
        "    Interweaving is always active when time_step is provided (which is\n",
        "    the case for all phases: training, validation, and inference via\n",
        "    CustomNetWithTimeStep).  When time_step is None the processor falls\n",
        "    back to the plain original adjacency as a safety net.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self._cayley_adj_cache: Dict[int, np.ndarray] = {}\n",
        "\n",
        "    def _get_cayley_adj(self, num_nodes: int) -> np.ndarray:\n",
        "        \"\"\"Get (or compute + cache) the Cayley adjacency for num_nodes.\"\"\"\n",
        "        if num_nodes not in self._cayley_adj_cache:\n",
        "            self._cayley_adj_cache[num_nodes] = build_truncated_cayley_adj(\n",
        "                num_nodes\n",
        "            )\n",
        "        return self._cayley_adj_cache[num_nodes]\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        node_fts: jnp.ndarray,   # (B, N, H)\n",
        "        edge_fts: jnp.ndarray,   # (B, N, N, H)\n",
        "        graph_fts: jnp.ndarray,  # (B, H)\n",
        "        adj_mat: jnp.ndarray,    # (B, N, N)  original graph\n",
        "        hidden: jnp.ndarray,     # (B, N, H)\n",
        "        **kwargs,\n",
        "    ) -> jnp.ndarray:\n",
        "        \"\"\"One message-passing step with adjacency interweaving.\n",
        "\n",
        "        Kwargs consumed:\n",
        "          time_step (int, traced): current algorithm time step\n",
        "          nb_nodes  (int, concrete): number of nodes in the graph\n",
        "        \"\"\"\n",
        "        time_step = kwargs.pop('time_step', None)\n",
        "        nb_nodes = kwargs.pop('nb_nodes', node_fts.shape[1])\n",
        "\n",
        "        if time_step is not None:\n",
        "            # --- Precompute Cayley adj (runs once at trace time) ---\n",
        "            cayley_adj_np = self._get_cayley_adj(nb_nodes)\n",
        "            cayley_adj = jnp.array(cayley_adj_np)                 # (N, N)\n",
        "            cayley_adj_batched = jnp.broadcast_to(\n",
        "                cayley_adj[None, :, :], adj_mat.shape              # (B, N, N)\n",
        "            )\n",
        "\n",
        "            # --- Select based on parity (evaluated at runtime via jnp.where) ---\n",
        "            is_even = (time_step % 2 == 0)\n",
        "            adj_mat_to_use = jnp.where(\n",
        "                is_even,          # condition (traced bool)\n",
        "                adj_mat,          # even  original graph\n",
        "                cayley_adj_batched  # odd   Cayley expander\n",
        "            )\n",
        "        else:\n",
        "            # Safety fallback: if time_step is not provided, use original adj.\n",
        "            # In practice this should not happen because CustomNetWithTimeStep\n",
        "            # always passes time_step for every phase (train / val / test).\n",
        "            adj_mat_to_use = adj_mat\n",
        "\n",
        "        return super().__call__(\n",
        "            node_fts, edge_fts, graph_fts, adj_mat_to_use, hidden, **kwargs\n",
        "        )\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Factory\n",
        "# ============================================================================\n",
        "\n",
        "def get_egp_factory(use_ln: bool, nb_triplet_fts: int, nb_heads=None):\n",
        "    \"\"\"Factory function that creates EGP_Processor instances.\"\"\"\n",
        "    def _factory(out_size: int):\n",
        "        return EGP_Processor(\n",
        "            out_size=out_size,\n",
        "            msgs_mlp_sizes=[out_size, out_size],\n",
        "            use_ln=use_ln,\n",
        "            use_triplets=False,\n",
        "            nb_triplet_fts=0,\n",
        "        )\n",
        "    return _factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2043a849",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Custom Net that passes time_step to processor for parity-based matrix selection\"\"\"\n",
        "\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import functools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from clrs._src import nets\n",
        "from clrs._src import decoders\n",
        "from clrs._src import encoders\n",
        "from clrs._src import probing\n",
        "from clrs._src import specs\n",
        "import haiku as hk\n",
        "\n",
        "_Array = jnp.ndarray\n",
        "_DataPoint = probing.DataPoint\n",
        "_Trajectory = List[_DataPoint]\n",
        "_Location = specs.Location\n",
        "_Spec = specs.Spec\n",
        "_Stage = specs.Stage\n",
        "_Type = specs.Type\n",
        "\n",
        "\n",
        "class CustomNetWithTimeStep(nets.Net):\n",
        "  \"\"\"Extended Net that passes time_step to processor for dynamic matrix selection\n",
        "  \n",
        "  This class extends the parent Net to pass the current time step (i) to the processor,\n",
        "  allowing it to make decisions based on time step parity or other time-dependent logic.\n",
        "  \"\"\"\n",
        "\n",
        "  def _msg_passing_step(self,\n",
        "                        mp_state: nets._MessagePassingScanState,\n",
        "                        i: int,  #  TIME STEP\n",
        "                        hints: List[_DataPoint],\n",
        "                        repred: bool,\n",
        "                        lengths: jnp.ndarray,\n",
        "                        batch_size: int,\n",
        "                        nb_nodes: int,\n",
        "                        inputs: _Trajectory,\n",
        "                        first_step: bool,\n",
        "                        spec: _Spec,\n",
        "                        encs: Dict[str, List[hk.Module]],\n",
        "                        decs: Dict[str, Tuple[hk.Module]],\n",
        "                        return_hints: bool,\n",
        "                        return_all_outputs: bool\n",
        "                        ):\n",
        "    \"\"\"Process one time step, passing the time step to the processor\"\"\"\n",
        "    \n",
        "    # Handle hint decoding (same as parent)\n",
        "    if self.decode_hints and not first_step:\n",
        "      assert self._hint_repred_mode in ['soft', 'hard', 'hard_on_eval']\n",
        "      hard_postprocess = (self._hint_repred_mode == 'hard' or\n",
        "                          (self._hint_repred_mode == 'hard_on_eval' and repred))\n",
        "      decoded_hint = decoders.postprocess(spec,\n",
        "                                          mp_state.hint_preds,\n",
        "                                          sinkhorn_temperature=0.1,\n",
        "                                          sinkhorn_steps=25,\n",
        "                                          hard=hard_postprocess)\n",
        "    \n",
        "    # Handle current hints (same as parent)\n",
        "    if repred and self.decode_hints and not first_step:\n",
        "      cur_hint = []\n",
        "      for hint in decoded_hint:\n",
        "        cur_hint.append(decoded_hint[hint])\n",
        "    else:\n",
        "      cur_hint = []\n",
        "      needs_noise = (self.decode_hints and not first_step and\n",
        "                     self._hint_teacher_forcing < 1.0)\n",
        "      if needs_noise:\n",
        "        force_mask = jax.random.bernoulli(\n",
        "            hk.next_rng_key(), self._hint_teacher_forcing,\n",
        "            (batch_size,))\n",
        "      else:\n",
        "        force_mask = None\n",
        "      for hint in hints:\n",
        "        hint_data = jnp.asarray(hint.data)[i]\n",
        "        _, loc, typ = spec[hint.name]\n",
        "        if needs_noise:\n",
        "          if (typ == _Type.POINTER and\n",
        "              decoded_hint[hint.name].type_ == _Type.SOFT_POINTER):\n",
        "            hint_data = hk.one_hot(hint_data, nb_nodes)\n",
        "            typ = _Type.SOFT_POINTER\n",
        "          hint_data = jnp.where(nets._expand_to(force_mask, hint_data),\n",
        "                                hint_data,\n",
        "                                decoded_hint[hint.name].data)\n",
        "        cur_hint.append(\n",
        "            probing.DataPoint(\n",
        "                name=hint.name, location=loc, type_=typ, data=hint_data))\n",
        "\n",
        "    # Call _one_step_pred with time_step (MODIFIED LINE)\n",
        "    hiddens, output_preds_cand, hint_preds, lstm_state = self._one_step_pred(\n",
        "        inputs, cur_hint, mp_state.hiddens,\n",
        "        batch_size, nb_nodes, mp_state.lstm_state,\n",
        "        spec, encs, decs, repred,\n",
        "        time_step=i)  #  PASS TIME STEP\n",
        "\n",
        "    # Handle output predictions (same as parent)\n",
        "    if first_step:\n",
        "      output_preds = output_preds_cand\n",
        "    else:\n",
        "      output_preds = {}\n",
        "      for outp in mp_state.output_preds:\n",
        "        is_not_done = nets._is_not_done_broadcast(lengths, i,\n",
        "                                                    output_preds_cand[outp])\n",
        "        output_preds[outp] = is_not_done * output_preds_cand[outp] + (\n",
        "            1.0 - is_not_done) * mp_state.output_preds[outp]\n",
        "\n",
        "    new_mp_state = nets._MessagePassingScanState(\n",
        "        hint_preds=hint_preds,\n",
        "        output_preds=output_preds,\n",
        "        hiddens=hiddens,\n",
        "        lstm_state=lstm_state)\n",
        "    \n",
        "    return new_mp_state, new_mp_state\n",
        "\n",
        "  def _one_step_pred(\n",
        "      self,\n",
        "      inputs: _Trajectory,\n",
        "      hints: _Trajectory,\n",
        "      hidden: _Array,\n",
        "      batch_size: int,\n",
        "      nb_nodes: int,\n",
        "      lstm_state: Optional[hk.LSTMState],\n",
        "      spec: _Spec,\n",
        "      encs: Dict[str, List[hk.Module]],\n",
        "      decs: Dict[str, Tuple[hk.Module]],\n",
        "      repred: bool,\n",
        "      time_step: Optional[int] = None,  #  ADD PARAMETER\n",
        "  ):\n",
        "    \"\"\"Generates one-step predictions with optional time_step awareness\"\"\"\n",
        "\n",
        "    # Initialize features (same as parent)\n",
        "    node_fts = jnp.zeros((batch_size, nb_nodes, self.hidden_dim))\n",
        "    edge_fts = jnp.zeros((batch_size, nb_nodes, nb_nodes, self.hidden_dim))\n",
        "    graph_fts = jnp.zeros((batch_size, self.hidden_dim))\n",
        "    adj_mat = jnp.repeat(\n",
        "        jnp.expand_dims(jnp.eye(nb_nodes), 0), batch_size, axis=0)\n",
        "\n",
        "    # ENCODE (same as parent)\n",
        "    trajectories = [inputs]\n",
        "    if self.encode_hints:\n",
        "      trajectories.append(hints)\n",
        "\n",
        "    for trajectory in trajectories:\n",
        "      for dp in trajectory:\n",
        "        try:\n",
        "          dp = encoders.preprocess(dp, nb_nodes)\n",
        "          assert dp.type_ != _Type.SOFT_POINTER\n",
        "          adj_mat = encoders.accum_adj_mat(dp, adj_mat)\n",
        "          encoder = encs[dp.name]\n",
        "          edge_fts = encoders.accum_edge_fts(encoder, dp, edge_fts)\n",
        "          node_fts = encoders.accum_node_fts(encoder, dp, node_fts)\n",
        "          graph_fts = encoders.accum_graph_fts(encoder, dp, graph_fts)\n",
        "        except Exception as e:\n",
        "          raise Exception(f'Failed to process {dp}') from e\n",
        "\n",
        "    # PROCESS (MODIFIED - pass time_step to processor)\n",
        "    nxt_hidden = hidden\n",
        "    for _ in range(self.nb_msg_passing_steps):\n",
        "      nxt_hidden, nxt_edge = self.processor(\n",
        "          node_fts,\n",
        "          edge_fts,\n",
        "          graph_fts,\n",
        "          adj_mat,\n",
        "          nxt_hidden,\n",
        "          batch_size=batch_size,\n",
        "          nb_nodes=nb_nodes,\n",
        "          time_step=time_step,  #  PASS TIME STEP HERE\n",
        "      )\n",
        "\n",
        "    # Rest is same as parent\n",
        "    if not repred:\n",
        "      nxt_hidden = hk.dropout(hk.next_rng_key(), self._dropout_prob, nxt_hidden)\n",
        "\n",
        "    if self.use_lstm:\n",
        "      nxt_hidden, nxt_lstm_state = jax.vmap(self.lstm)(nxt_hidden, lstm_state)\n",
        "    else:\n",
        "      nxt_lstm_state = None\n",
        "\n",
        "    h_t = jnp.concatenate([node_fts, hidden, nxt_hidden], axis=-1)\n",
        "    if nxt_edge is not None:\n",
        "      e_t = jnp.concatenate([edge_fts, nxt_edge], axis=-1)\n",
        "    else:\n",
        "      e_t = edge_fts\n",
        "\n",
        "    # DECODE\n",
        "    hint_preds, output_preds = decoders.decode_fts(\n",
        "        decoders=decs,\n",
        "        spec=spec,\n",
        "        h_t=h_t,\n",
        "        adj_mat=adj_mat,\n",
        "        edge_fts=e_t,\n",
        "        graph_fts=graph_fts,\n",
        "        inf_bias=self.processor.inf_bias,\n",
        "        inf_bias_edge=self.processor.inf_bias_edge,\n",
        "        repred=repred,\n",
        "    )\n",
        "\n",
        "    return nxt_hidden, output_preds, hint_preds, nxt_lstm_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9b901628",
      "metadata": {},
      "outputs": [],
      "source": [
        "import clrs\n",
        "import haiku as hk\n",
        "import functools\n",
        "import numpy as np\n",
        "import jax\n",
        "import logging\n",
        "\n",
        "def egp_clrs(seed):\n",
        "  wandb.init(project=\"clrs-project\", name=f\"egp-training-{FLAGS.algorithms}\")\n",
        "  \n",
        "  if FLAGS.hint_mode == 'encoded_decoded':\n",
        "    encode_hints = True\n",
        "    decode_hints = True\n",
        "  elif FLAGS.hint_mode == 'decoded_only':\n",
        "    encode_hints = False\n",
        "    decode_hints = True\n",
        "  elif FLAGS.hint_mode == 'none':\n",
        "    encode_hints = False\n",
        "    decode_hints = False\n",
        "  else:\n",
        "    raise ValueError('Hint mode not in {encoded_decoded, decoded_only, none}.')\n",
        "\n",
        "  train_lengths = [int(x) for x in FLAGS.train_lengths]\n",
        "  rng = np.random.RandomState(seed)\n",
        "  rng_key = jax.random.PRNGKey(rng.randint(2**32))\n",
        "\n",
        "  print(\"------> Creating samplers\")\n",
        "  (\n",
        "      train_samplers,\n",
        "      val_samplers,\n",
        "      val_sample_counts,\n",
        "      test_samplers,\n",
        "      test_sample_counts,\n",
        "      spec_list,\n",
        "  ) = create_samplers(\n",
        "      rng=rng,\n",
        "      train_lengths=train_lengths,\n",
        "      algorithms=FLAGS.algorithms,\n",
        "      val_lengths=[np.amax(train_lengths)],\n",
        "      test_lengths=[-1],\n",
        "      train_batch_size=FLAGS.batch_size,\n",
        "  )\n",
        "\n",
        "  # ============================================================================\n",
        "  # EGP processor factory  interweaving is always active.\n",
        "  # CustomNetWithTimeStep passes time_step to the processor in every phase\n",
        "  # (training, validation, inference), so the Cayley adjacency interweaving\n",
        "  # is consistently used throughout.\n",
        "  # ============================================================================\n",
        "  processor_factory = get_egp_factory(\n",
        "      use_ln=FLAGS.use_ln,\n",
        "      nb_triplet_fts=FLAGS.nb_triplet_fts,\n",
        "      nb_heads=FLAGS.nb_heads,\n",
        "  )\n",
        "\n",
        "  model_params = dict(\n",
        "      processor_factory=processor_factory,\n",
        "      hidden_dim=FLAGS.hidden_size,\n",
        "      encode_hints=encode_hints,\n",
        "      decode_hints=decode_hints,\n",
        "      encoder_init=FLAGS.encoder_init,\n",
        "      use_lstm=FLAGS.use_lstm,\n",
        "      learning_rate=FLAGS.learning_rate,\n",
        "      grad_clip_max_norm=FLAGS.grad_clip_max_norm,\n",
        "      checkpoint_path=FLAGS.checkpoint_path,\n",
        "      freeze_processor=FLAGS.freeze_processor,\n",
        "      dropout_prob=FLAGS.dropout_prob,\n",
        "      hint_teacher_forcing=FLAGS.hint_teacher_forcing,\n",
        "      hint_repred_mode=FLAGS.hint_repred_mode,\n",
        "      nb_msg_passing_steps=FLAGS.nb_msg_passing_steps,\n",
        "  )\n",
        "\n",
        "  print(\"------> Creating model\")\n",
        "  # Custom BaselineModel that uses CustomNetWithTimeStep so the processor\n",
        "  # always receives time_step for Cayley adjacency interweaving.\n",
        "  class CustomBaselineModel(clrs.models.BaselineModel):\n",
        "    def _create_net_fns(self, hidden_dim, encode_hints, processor_factory,\n",
        "                        use_lstm, encoder_init, dropout_prob,\n",
        "                        hint_teacher_forcing, hint_repred_mode):\n",
        "      \"\"\"Override to use CustomNetWithTimeStep instead of Net.\"\"\"\n",
        "      def _use_net(*args, **kwargs):\n",
        "        return CustomNetWithTimeStep(\n",
        "            self._spec, hidden_dim, encode_hints, self.decode_hints,\n",
        "            processor_factory, use_lstm, encoder_init,\n",
        "            dropout_prob, hint_teacher_forcing,\n",
        "            hint_repred_mode,\n",
        "            self.nb_dims, self.nb_msg_passing_steps,\n",
        "            self.debug)(*args, **kwargs)\n",
        "\n",
        "      self.net_fn = hk.transform(_use_net)\n",
        "\n",
        "      pmap_args = dict(axis_name='batch', devices=jax.local_devices())\n",
        "      n_devices = jax.local_device_count()\n",
        "      func, static_arg, extra_args = (\n",
        "          (jax.jit, 'static_argnums', {}) if n_devices == 1 else\n",
        "          (jax.pmap, 'static_broadcasted_argnums', pmap_args))\n",
        "      pmean = functools.partial(jax.lax.pmean, axis_name='batch')\n",
        "      self._maybe_pmean = pmean if n_devices > 1 else lambda x: x\n",
        "      extra_args[static_arg] = 3\n",
        "      self.jitted_grad = func(self._compute_grad, **extra_args)\n",
        "      extra_args[static_arg] = 4\n",
        "      self.jitted_feedback = func(self._feedback, donate_argnums=[0, 3],\n",
        "                                  **extra_args)\n",
        "      extra_args[static_arg] = [3, 4, 5]\n",
        "      self.jitted_predict = func(self._predict, **extra_args)\n",
        "      extra_args[static_arg] = [3, 4]\n",
        "\n",
        "  # Single model for training + evaluation (like vanilla CLRS).\n",
        "  # Interweaving is always active via CustomNetWithTimeStep  EGP_Processor.\n",
        "  eval_model = CustomBaselineModel(\n",
        "      spec=spec_list,\n",
        "      dummy_trajectory=[next(t) for t in val_samplers],\n",
        "      **model_params\n",
        "  )\n",
        "  if FLAGS.chunked_training:\n",
        "    train_model = CustomBaselineModel(\n",
        "        spec=spec_list,\n",
        "        dummy_trajectory=[next(t) for t in train_samplers],\n",
        "        **model_params\n",
        "    )\n",
        "  else:\n",
        "    train_model = eval_model\n",
        "\n",
        "  # ========================================================================\n",
        "  # Training loop\n",
        "  # ========================================================================\n",
        "  print(\"------> Training loop initialization\")\n",
        "  best_score = -1.0\n",
        "  current_train_items = [0] * len(FLAGS.algorithms)\n",
        "  step = 0\n",
        "  next_eval = 0\n",
        "  val_scores = [-99999.9] * len(FLAGS.algorithms)\n",
        "  length_idx = 0\n",
        "\n",
        "  while step < FLAGS.train_steps:\n",
        "    feedback_list = [next(t) for t in train_samplers]\n",
        "\n",
        "    # Initialize model on the first step.\n",
        "    if step == 0:\n",
        "      all_features = [f.features for f in feedback_list]\n",
        "      if FLAGS.chunked_training:\n",
        "        all_length_features = [all_features] + [\n",
        "            [next(t).features for t in train_samplers]\n",
        "            for _ in range(len(train_lengths))]\n",
        "        train_model.init(all_length_features[:-1], FLAGS.seed + 1)\n",
        "      else:\n",
        "        train_model.init(all_features, FLAGS.seed + 1)\n",
        "\n",
        "    # Training step.\n",
        "    for algo_idx in range(len(train_samplers)):\n",
        "      feedback = feedback_list[algo_idx]\n",
        "      rng_key, new_rng_key = jax.random.split(rng_key)\n",
        "      if FLAGS.chunked_training:\n",
        "        length_and_algo_idx = (length_idx, algo_idx)\n",
        "      else:\n",
        "        length_and_algo_idx = algo_idx\n",
        "\n",
        "      cur_loss = train_model.feedback(rng_key, feedback, length_and_algo_idx)\n",
        "      rng_key = new_rng_key\n",
        "\n",
        "      if FLAGS.chunked_training:\n",
        "        examples_in_chunk = np.sum(feedback.features.is_last).item()\n",
        "      else:\n",
        "        examples_in_chunk = len(feedback.features.lengths)\n",
        "      current_train_items[algo_idx] += examples_in_chunk\n",
        "      logging.info('Algo %s step %i current loss %f, current_train_items %i.',\n",
        "                   FLAGS.algorithms[algo_idx], step,\n",
        "                   cur_loss, current_train_items[algo_idx])\n",
        "      wandb.log({\"loss\": float(cur_loss), \"step\": step})\n",
        "\n",
        "    # Periodically evaluate model.\n",
        "    if step >= next_eval:\n",
        "      eval_model.params = train_model.params\n",
        "      for algo_idx in range(len(train_samplers)):\n",
        "        common_extras = {'examples_seen': current_train_items[algo_idx],\n",
        "                         'step': step,\n",
        "                         'algorithm': FLAGS.algorithms[algo_idx]}\n",
        "\n",
        "        new_rng_key, rng_key = jax.random.split(rng_key)\n",
        "        val_stats = collect_and_eval(\n",
        "            val_samplers[algo_idx],\n",
        "            functools.partial(eval_model.predict, algorithm_index=algo_idx),\n",
        "            val_sample_counts[algo_idx],\n",
        "            new_rng_key,\n",
        "            extras=common_extras)\n",
        "        logging.info('(val) algo %s step %d: %s',\n",
        "                     FLAGS.algorithms[algo_idx], step, val_stats)\n",
        "        val_scores[algo_idx] = val_stats['score']\n",
        "\n",
        "      next_eval += FLAGS.eval_every\n",
        "\n",
        "      msg = (f'best avg val score was '\n",
        "             f'{best_score/len(FLAGS.algorithms):.3f}, '\n",
        "             f'current avg val score is {np.mean(val_scores):.3f}, '\n",
        "             f'val scores are: ')\n",
        "      msg += ', '.join(\n",
        "          ['%s: %.3f' % (x, y) for (x, y) in zip(FLAGS.algorithms, val_scores)])\n",
        "      if (sum(val_scores) > best_score) or step == 0:\n",
        "        best_score = sum(val_scores)\n",
        "        logging.info('Checkpointing best model, %s', msg)\n",
        "        train_model.save_model('best.pkl')\n",
        "      else:\n",
        "        logging.info('Not saving new best model, %s', msg)\n",
        "\n",
        "      wandb.log({\"score\": float(np.mean(val_scores)), \"step\": step})\n",
        "\n",
        "    step += 1\n",
        "    length_idx = (length_idx + 1) % len(train_lengths)\n",
        "\n",
        "  # ========================================================================\n",
        "  # Testing\n",
        "  # ========================================================================\n",
        "  logging.info('Restoring best model from checkpoint...')\n",
        "  eval_model.restore_model('best.pkl', only_load_processor=False)\n",
        "\n",
        "  for algo_idx in range(len(train_samplers)):\n",
        "    common_extras = {'examples_seen': current_train_items[algo_idx],\n",
        "                     'step': step,\n",
        "                     'algorithm': FLAGS.algorithms[algo_idx]}\n",
        "\n",
        "    new_rng_key, rng_key = jax.random.split(rng_key)\n",
        "    test_stats = collect_and_eval(\n",
        "        test_samplers[algo_idx],\n",
        "        functools.partial(eval_model.predict, algorithm_index=algo_idx),\n",
        "        test_sample_counts[algo_idx],\n",
        "        new_rng_key,\n",
        "        extras=common_extras)\n",
        "    logging.info('(test) algo %s : %s', FLAGS.algorithms[algo_idx], test_stats)\n",
        "\n",
        "  logging.info('Done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "630075ef",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td></td></tr><tr><td>score</td><td></td></tr><tr><td>step</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>11.03319</td></tr><tr><td>score</td><td>0.08228</td></tr><tr><td>step</td><td>199</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">baseline-training-['kmp_matcher']-mpnn</strong> at: <a href='https://wandb.ai/tgoabike/clrs-project/runs/v0ciznfi' target=\"_blank\">https://wandb.ai/tgoabike/clrs-project/runs/v0ciznfi</a><br> View project at: <a href='https://wandb.ai/tgoabike/clrs-project' target=\"_blank\">https://wandb.ai/tgoabike/clrs-project</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20260211_124339-v0ciznfi/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.24.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/juli/Documents/cs/machine learning/GNNs/cayleydoscope/cayleydoscope/clrs/examples/wandb/run-20260211_132603-td3w6ttk</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tgoabike/clrs-project/runs/td3w6ttk' target=\"_blank\">egp-training-['kmp_matcher']</a></strong> to <a href='https://wandb.ai/tgoabike/clrs-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/tgoabike/clrs-project' target=\"_blank\">https://wandb.ai/tgoabike/clrs-project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/tgoabike/clrs-project/runs/td3w6ttk' target=\"_blank\">https://wandb.ai/tgoabike/clrs-project/runs/td3w6ttk</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Ignoring kwargs {'p'} when building sampler class <class 'clrs._src.samplers.MatcherSampler'>\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------> Creating samplers\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Ignoring kwargs {'p'} when building sampler class <class 'clrs._src.samplers.MatcherSampler'>\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------> Creating model\n",
            "------> Training loop initialization\n"
          ]
        }
      ],
      "source": [
        "egp_clrs(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6b748c59",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CGP (Cayley Graph Propagation) Processor\n",
        "# ============================================================================\n",
        "#\n",
        "# Key difference from EGP:\n",
        "#   EGP truncates the Cayley graph to match the input graph's node count,\n",
        "#   which can damage the desirable spectral expansion properties.\n",
        "#\n",
        "#   CGP uses the COMPLETE Cayley graph by introducing virtual nodes,\n",
        "#   preserving the full expander structure. Virtual nodes are padded\n",
        "#   with zero features and removed after message passing.\n",
        "#\n",
        "# Interweaving strategy (same parity rule as EGP):\n",
        "#   Even time steps  original adj_mat (padded, virtual nodes have self-loops)\n",
        "#   Odd  time steps  complete Cayley adjacency (all N_cayley nodes connected)\n",
        "# ============================================================================\n",
        "\n",
        "class CGP_Processor(processors.PGN):\n",
        "    \"\"\"Cayley Graph Propagation processor with virtual nodes.\n",
        "\n",
        "    Unlike EGP, which truncates the Cayley graph to match the number of\n",
        "    input nodes (potentially breaking expansion properties), CGP pads\n",
        "    the input graph to the Cayley graph size with virtual nodes and\n",
        "    uses the *complete* Cayley structure.\n",
        "\n",
        "    For each message-passing call:\n",
        "      1. Pad node_fts, edge_fts, adj_mat, hidden  N_cayley with virtual nodes\n",
        "      2. Select adjacency:\n",
        "         - even time_step  original adj (padded, virtual nodes self-loop only)\n",
        "         - odd  time_step  complete Cayley graph adjacency\n",
        "      3. Run PGN message passing on the expanded graph\n",
        "      4. Truncate output back to N (discard virtual node representations)\n",
        "\n",
        "    Virtual node features are zero-initialized each step, acting as\n",
        "    communication conduits through the Cayley expander topology.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self._cayley_cache: Dict[int, tuple] = {}\n",
        "\n",
        "    def _build_cayley(self, num_nodes: int):\n",
        "        \"\"\"Build and cache the complete Cayley graph adjacency + metadata.\n",
        "\n",
        "        Returns:\n",
        "            (cayley_adj_np, cayley_n_nodes, n_virtual, virtual_self_loop_np)\n",
        "        \"\"\"\n",
        "        if num_nodes not in self._cayley_cache:\n",
        "            n = get_cayley_n(num_nodes)\n",
        "            cayley_n_nodes = cayley_graph_size(n)\n",
        "            edge_index = get_cayley_graph(n)\n",
        "\n",
        "            # Complete Cayley adjacency with self-loops\n",
        "            cayley_adj = np.zeros((cayley_n_nodes, cayley_n_nodes), dtype=np.float32)\n",
        "            cayley_adj[edge_index[0], edge_index[1]] = 1.0\n",
        "            np.fill_diagonal(cayley_adj, 1.0)\n",
        "\n",
        "            # Self-loop matrix for virtual nodes only (used with padded original adj)\n",
        "            n_virtual = cayley_n_nodes - num_nodes\n",
        "            virtual_self_loop = np.zeros((cayley_n_nodes, cayley_n_nodes), dtype=np.float32)\n",
        "            if n_virtual > 0:\n",
        "                np.fill_diagonal(virtual_self_loop[num_nodes:, num_nodes:], 1.0)\n",
        "\n",
        "            self._cayley_cache[num_nodes] = (\n",
        "                cayley_adj, cayley_n_nodes, n_virtual, virtual_self_loop\n",
        "            )\n",
        "        return self._cayley_cache[num_nodes]\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        node_fts: jnp.ndarray,   # (B, N, H)\n",
        "        edge_fts: jnp.ndarray,   # (B, N, N, H)\n",
        "        graph_fts: jnp.ndarray,  # (B, H)\n",
        "        adj_mat: jnp.ndarray,    # (B, N, N)\n",
        "        hidden: jnp.ndarray,     # (B, N, H)\n",
        "        **kwargs,\n",
        "    ) -> jnp.ndarray:\n",
        "        \"\"\"Message-passing step with virtual node expansion and Cayley interweaving.\"\"\"\n",
        "        time_step = kwargs.pop('time_step', None)\n",
        "        nb_nodes = kwargs.pop('nb_nodes', node_fts.shape[1])\n",
        "\n",
        "        cayley_adj_np, cayley_n_nodes, n_virtual, virtual_self_loop_np = \\\n",
        "            self._build_cayley(nb_nodes)\n",
        "\n",
        "        # ---- Pad spatial dimensions from N  N_cayley ----\n",
        "        if n_virtual > 0:\n",
        "            # Pad node features:  (B, N, H)  (B, N_cayley, H)\n",
        "            node_fts_padded = jnp.pad(\n",
        "                node_fts, ((0, 0), (0, n_virtual), (0, 0)))\n",
        "            # Pad hidden state:   (B, N, H)  (B, N_cayley, H)\n",
        "            hidden_padded = jnp.pad(\n",
        "                hidden, ((0, 0), (0, n_virtual), (0, 0)))\n",
        "            # Pad edge features:  (B, N, N, H)  (B, N_cayley, N_cayley, H)\n",
        "            edge_fts_padded = jnp.pad(\n",
        "                edge_fts, ((0, 0), (0, n_virtual), (0, n_virtual), (0, 0)))\n",
        "            # Pad adj_mat + add self-loops for virtual nodes\n",
        "            adj_padded = jnp.pad(\n",
        "                adj_mat, ((0, 0), (0, n_virtual), (0, n_virtual)))\n",
        "            adj_padded = adj_padded + jnp.array(virtual_self_loop_np)[None]\n",
        "        else:\n",
        "            node_fts_padded = node_fts\n",
        "            hidden_padded = hidden\n",
        "            edge_fts_padded = edge_fts\n",
        "            adj_padded = adj_mat\n",
        "\n",
        "        # ---- Select adjacency based on time-step parity ----\n",
        "        if time_step is not None:\n",
        "            cayley_adj = jnp.array(cayley_adj_np)\n",
        "            cayley_adj_batched = jnp.broadcast_to(\n",
        "                cayley_adj[None, :, :], adj_padded.shape\n",
        "            )\n",
        "            is_even = (time_step % 2 == 0)\n",
        "            adj_to_use = jnp.where(is_even, adj_padded, cayley_adj_batched)\n",
        "        else:\n",
        "            adj_to_use = adj_padded\n",
        "\n",
        "        # ---- Run PGN on expanded graph ----\n",
        "        result, tri_msgs = super().__call__(\n",
        "            node_fts_padded, edge_fts_padded, graph_fts,\n",
        "            adj_to_use, hidden_padded, **kwargs\n",
        "        )\n",
        "\n",
        "        # ---- Truncate back to original N nodes ----\n",
        "        if n_virtual > 0:\n",
        "            result = result[:, :nb_nodes, :]\n",
        "            if tri_msgs is not None:\n",
        "                tri_msgs = tri_msgs[:, :nb_nodes, :nb_nodes, :]\n",
        "\n",
        "        return result, tri_msgs\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CGP Factory\n",
        "# ============================================================================\n",
        "\n",
        "def get_cgp_factory(use_ln: bool, nb_triplet_fts: int, nb_heads=None):\n",
        "    \"\"\"Factory function that creates CGP_Processor instances.\"\"\"\n",
        "    def _factory(out_size: int):\n",
        "        return CGP_Processor(\n",
        "            out_size=out_size,\n",
        "            msgs_mlp_sizes=[out_size, out_size],\n",
        "            use_ln=use_ln,\n",
        "            use_triplets=False,\n",
        "            nb_triplet_fts=0,\n",
        "        )\n",
        "    return _factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9fcbe568",
      "metadata": {},
      "outputs": [],
      "source": [
        "import clrs\n",
        "import haiku as hk\n",
        "import functools\n",
        "import numpy as np\n",
        "import jax\n",
        "import logging\n",
        "\n",
        "def cgp_clrs(seed):\n",
        "  \"\"\"Train a CLRS model using Cayley Graph Propagation (CGP).\n",
        "\n",
        "  CGP differs from EGP by using the complete Cayley graph with virtual\n",
        "  nodes rather than truncating the Cayley graph. This preserves the\n",
        "  full spectral expansion properties of the Cayley structure.\n",
        "\n",
        "  Uses CustomNetWithTimeStep (defined in Cell 9) so the processor\n",
        "  always receives time_step for adjacency interweaving.\n",
        "  \"\"\"\n",
        "  wandb.init(project=\"clrs-project\", name=f\"cgp-training-{FLAGS.algorithms}\")\n",
        "\n",
        "  if FLAGS.hint_mode == 'encoded_decoded':\n",
        "    encode_hints = True\n",
        "    decode_hints = True\n",
        "  elif FLAGS.hint_mode == 'decoded_only':\n",
        "    encode_hints = False\n",
        "    decode_hints = True\n",
        "  elif FLAGS.hint_mode == 'none':\n",
        "    encode_hints = False\n",
        "    decode_hints = False\n",
        "  else:\n",
        "    raise ValueError('Hint mode not in {encoded_decoded, decoded_only, none}.')\n",
        "\n",
        "  train_lengths = [int(x) for x in FLAGS.train_lengths]\n",
        "  rng = np.random.RandomState(seed)\n",
        "  rng_key = jax.random.PRNGKey(rng.randint(2**32))\n",
        "\n",
        "  print(\"------> [CGP] Creating samplers\")\n",
        "  (\n",
        "      train_samplers,\n",
        "      val_samplers,\n",
        "      val_sample_counts,\n",
        "      test_samplers,\n",
        "      test_sample_counts,\n",
        "      spec_list,\n",
        "  ) = create_samplers(\n",
        "      rng=rng,\n",
        "      train_lengths=train_lengths,\n",
        "      algorithms=FLAGS.algorithms,\n",
        "      val_lengths=[np.amax(train_lengths)],\n",
        "      test_lengths=[-1],\n",
        "      train_batch_size=FLAGS.batch_size,\n",
        "  )\n",
        "\n",
        "  # ============================================================================\n",
        "  # CGP processor factory  uses complete Cayley graph with virtual nodes.\n",
        "  # CustomNetWithTimeStep passes time_step so the processor can interweave.\n",
        "  # ============================================================================\n",
        "  processor_factory = get_cgp_factory(\n",
        "      use_ln=FLAGS.use_ln,\n",
        "      nb_triplet_fts=FLAGS.nb_triplet_fts,\n",
        "      nb_heads=FLAGS.nb_heads,\n",
        "  )\n",
        "\n",
        "  model_params = dict(\n",
        "      processor_factory=processor_factory,\n",
        "      hidden_dim=FLAGS.hidden_size,\n",
        "      encode_hints=encode_hints,\n",
        "      decode_hints=decode_hints,\n",
        "      encoder_init=FLAGS.encoder_init,\n",
        "      use_lstm=FLAGS.use_lstm,\n",
        "      learning_rate=FLAGS.learning_rate,\n",
        "      grad_clip_max_norm=FLAGS.grad_clip_max_norm,\n",
        "      checkpoint_path=FLAGS.checkpoint_path,\n",
        "      freeze_processor=FLAGS.freeze_processor,\n",
        "      dropout_prob=FLAGS.dropout_prob,\n",
        "      hint_teacher_forcing=FLAGS.hint_teacher_forcing,\n",
        "      hint_repred_mode=FLAGS.hint_repred_mode,\n",
        "      nb_msg_passing_steps=FLAGS.nb_msg_passing_steps,\n",
        "  )\n",
        "\n",
        "  print(\"------> [CGP] Creating model\")\n",
        "\n",
        "  # Custom BaselineModel that uses CustomNetWithTimeStep so the processor\n",
        "  # always receives time_step for Cayley adjacency interweaving.\n",
        "  class CGPBaselineModel(clrs.models.BaselineModel):\n",
        "    def _create_net_fns(self, hidden_dim, encode_hints, processor_factory,\n",
        "                        use_lstm, encoder_init, dropout_prob,\n",
        "                        hint_teacher_forcing, hint_repred_mode):\n",
        "      \"\"\"Override to use CustomNetWithTimeStep instead of Net.\"\"\"\n",
        "      def _use_net(*args, **kwargs):\n",
        "        return CustomNetWithTimeStep(\n",
        "            self._spec, hidden_dim, encode_hints, self.decode_hints,\n",
        "            processor_factory, use_lstm, encoder_init,\n",
        "            dropout_prob, hint_teacher_forcing,\n",
        "            hint_repred_mode,\n",
        "            self.nb_dims, self.nb_msg_passing_steps,\n",
        "            self.debug)(*args, **kwargs)\n",
        "\n",
        "      self.net_fn = hk.transform(_use_net)\n",
        "\n",
        "      pmap_args = dict(axis_name='batch', devices=jax.local_devices())\n",
        "      n_devices = jax.local_device_count()\n",
        "      func, static_arg, extra_args = (\n",
        "          (jax.jit, 'static_argnums', {}) if n_devices == 1 else\n",
        "          (jax.pmap, 'static_broadcasted_argnums', pmap_args))\n",
        "      pmean = functools.partial(jax.lax.pmean, axis_name='batch')\n",
        "      self._maybe_pmean = pmean if n_devices > 1 else lambda x: x\n",
        "      extra_args[static_arg] = 3\n",
        "      self.jitted_grad = func(self._compute_grad, **extra_args)\n",
        "      extra_args[static_arg] = 4\n",
        "      self.jitted_feedback = func(self._feedback, donate_argnums=[0, 3],\n",
        "                                  **extra_args)\n",
        "      extra_args[static_arg] = [3, 4, 5]\n",
        "      self.jitted_predict = func(self._predict, **extra_args)\n",
        "      extra_args[static_arg] = [3, 4]\n",
        "\n",
        "  eval_model = CGPBaselineModel(\n",
        "      spec=spec_list,\n",
        "      dummy_trajectory=[next(t) for t in val_samplers],\n",
        "      **model_params\n",
        "  )\n",
        "  if FLAGS.chunked_training:\n",
        "    train_model = CGPBaselineModel(\n",
        "        spec=spec_list,\n",
        "        dummy_trajectory=[next(t) for t in train_samplers],\n",
        "        **model_params\n",
        "    )\n",
        "  else:\n",
        "    train_model = eval_model\n",
        "\n",
        "  # ========================================================================\n",
        "  # Training loop\n",
        "  # ========================================================================\n",
        "  print(\"------> [CGP] Training loop initialization\")\n",
        "  best_score = -1.0\n",
        "  current_train_items = [0] * len(FLAGS.algorithms)\n",
        "  step = 0\n",
        "  next_eval = 0\n",
        "  val_scores = [-99999.9] * len(FLAGS.algorithms)\n",
        "  length_idx = 0\n",
        "\n",
        "  while step < FLAGS.train_steps:\n",
        "    feedback_list = [next(t) for t in train_samplers]\n",
        "\n",
        "    # Initialize model on the first step.\n",
        "    if step == 0:\n",
        "      all_features = [f.features for f in feedback_list]\n",
        "      if FLAGS.chunked_training:\n",
        "        all_length_features = [all_features] + [\n",
        "            [next(t).features for t in train_samplers]\n",
        "            for _ in range(len(train_lengths))]\n",
        "        train_model.init(all_length_features[:-1], FLAGS.seed + 1)\n",
        "      else:\n",
        "        train_model.init(all_features, FLAGS.seed + 1)\n",
        "\n",
        "    # Training step.\n",
        "    for algo_idx in range(len(train_samplers)):\n",
        "      feedback = feedback_list[algo_idx]\n",
        "      rng_key, new_rng_key = jax.random.split(rng_key)\n",
        "      if FLAGS.chunked_training:\n",
        "        length_and_algo_idx = (length_idx, algo_idx)\n",
        "      else:\n",
        "        length_and_algo_idx = algo_idx\n",
        "\n",
        "      cur_loss = train_model.feedback(rng_key, feedback, length_and_algo_idx)\n",
        "      rng_key = new_rng_key\n",
        "\n",
        "      if FLAGS.chunked_training:\n",
        "        examples_in_chunk = np.sum(feedback.features.is_last).item()\n",
        "      else:\n",
        "        examples_in_chunk = len(feedback.features.lengths)\n",
        "      current_train_items[algo_idx] += examples_in_chunk\n",
        "      logging.info('[CGP] Algo %s step %i current loss %f, current_train_items %i.',\n",
        "                   FLAGS.algorithms[algo_idx], step,\n",
        "                   cur_loss, current_train_items[algo_idx])\n",
        "      wandb.log({\"loss\": float(cur_loss), \"step\": step})\n",
        "\n",
        "    # Periodically evaluate model.\n",
        "    if step >= next_eval:\n",
        "      eval_model.params = train_model.params\n",
        "      for algo_idx in range(len(train_samplers)):\n",
        "        common_extras = {'examples_seen': current_train_items[algo_idx],\n",
        "                         'step': step,\n",
        "                         'algorithm': FLAGS.algorithms[algo_idx]}\n",
        "\n",
        "        new_rng_key, rng_key = jax.random.split(rng_key)\n",
        "        val_stats = collect_and_eval(\n",
        "            val_samplers[algo_idx],\n",
        "            functools.partial(eval_model.predict, algorithm_index=algo_idx),\n",
        "            val_sample_counts[algo_idx],\n",
        "            new_rng_key,\n",
        "            extras=common_extras)\n",
        "        logging.info('[CGP] (val) algo %s step %d: %s',\n",
        "                     FLAGS.algorithms[algo_idx], step, val_stats)\n",
        "        val_scores[algo_idx] = val_stats['score']\n",
        "\n",
        "      next_eval += FLAGS.eval_every\n",
        "\n",
        "      msg = (f'best avg val score was '\n",
        "             f'{best_score/len(FLAGS.algorithms):.3f}, '\n",
        "             f'current avg val score is {np.mean(val_scores):.3f}, '\n",
        "             f'val scores are: ')\n",
        "      msg += ', '.join(\n",
        "          ['%s: %.3f' % (x, y) for (x, y) in zip(FLAGS.algorithms, val_scores)])\n",
        "      if (sum(val_scores) > best_score) or step == 0:\n",
        "        best_score = sum(val_scores)\n",
        "        logging.info('[CGP] Checkpointing best model, %s', msg)\n",
        "        train_model.save_model('best.pkl')\n",
        "      else:\n",
        "        logging.info('[CGP] Not saving new best model, %s', msg)\n",
        "\n",
        "      wandb.log({\"score\": float(np.mean(val_scores)), \"step\": step})\n",
        "\n",
        "    step += 1\n",
        "    length_idx = (length_idx + 1) % len(train_lengths)\n",
        "\n",
        "  # ========================================================================\n",
        "  # Testing\n",
        "  # ========================================================================\n",
        "  logging.info('[CGP] Restoring best model from checkpoint...')\n",
        "  eval_model.restore_model('best.pkl', only_load_processor=False)\n",
        "\n",
        "  for algo_idx in range(len(train_samplers)):\n",
        "    common_extras = {'examples_seen': current_train_items[algo_idx],\n",
        "                     'step': step,\n",
        "                     'algorithm': FLAGS.algorithms[algo_idx]}\n",
        "\n",
        "    new_rng_key, rng_key = jax.random.split(rng_key)\n",
        "    test_stats = collect_and_eval(\n",
        "        test_samplers[algo_idx],\n",
        "        functools.partial(eval_model.predict, algorithm_index=algo_idx),\n",
        "        test_sample_counts[algo_idx],\n",
        "        new_rng_key,\n",
        "        extras=common_extras)\n",
        "    logging.info('[CGP] (test) algo %s : %s', FLAGS.algorithms[algo_idx], test_stats)\n",
        "\n",
        "  logging.info('[CGP] Done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20563448",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td></td></tr><tr><td>score</td><td></td></tr><tr><td>step</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>16.31502</td></tr><tr><td>score</td><td>0.13013</td></tr><tr><td>step</td><td>199</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">egp-training-['kmp_matcher']</strong> at: <a href='https://wandb.ai/tgoabike/clrs-project/runs/td3w6ttk' target=\"_blank\">https://wandb.ai/tgoabike/clrs-project/runs/td3w6ttk</a><br> View project at: <a href='https://wandb.ai/tgoabike/clrs-project' target=\"_blank\">https://wandb.ai/tgoabike/clrs-project</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20260211_132603-td3w6ttk/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.24.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/juli/Documents/cs/machine learning/GNNs/cayleydoscope/cayleydoscope/clrs/examples/wandb/run-20260211_141728-5fri88wf</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tgoabike/clrs-project/runs/5fri88wf' target=\"_blank\">cgp-training-['kmp_matcher']</a></strong> to <a href='https://wandb.ai/tgoabike/clrs-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/tgoabike/clrs-project' target=\"_blank\">https://wandb.ai/tgoabike/clrs-project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/tgoabike/clrs-project/runs/5fri88wf' target=\"_blank\">https://wandb.ai/tgoabike/clrs-project/runs/5fri88wf</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Ignoring kwargs {'p'} when building sampler class <class 'clrs._src.samplers.MatcherSampler'>\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------> [CGP] Creating samplers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Ignoring kwargs {'p'} when building sampler class <class 'clrs._src.samplers.MatcherSampler'>\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------> [CGP] Creating model\n",
            "------> [CGP] Training loop initialization\n"
          ]
        }
      ],
      "source": [
        "cgp_clrs(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9592cce",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cayleydoscope",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
